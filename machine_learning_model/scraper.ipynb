{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuN1A0iEq-7D",
        "outputId": "949fffda-a181-4d77-9018-bef8e252e166"
      },
      "source": [
        "pip install python-docx\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=5c0002462e98c42d46a1fcdf6ed57fe3c0af0f589c67e978cedc63d128343bc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvytHRjhrGv1",
        "outputId": "71414030-013c-458b-88df-1293d338f15e"
      },
      "source": [
        "pip install scipy\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHvqfUoaItu7",
        "outputId": "4a9576f3-0702-40ba-ae18-0b84114d4c57"
      },
      "source": [
        "pip install mypy\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mypy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/53/8f73bf4726e42d049ef4855c5f047c9b12c4ac372ecfe38c761d4986296b/mypy-0.800-cp36-cp36m-manylinux2010_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from mypy) (3.7.4.3)\n",
            "Collecting typed-ast<1.5.0,>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/c1/4cc3c0da2374963f59b2f57ef02e048cdc4f609cbc1184b4146d0812e5b5/typed_ast-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 35.1MB/s \n",
            "\u001b[?25hCollecting mypy-extensions<0.5.0,>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Installing collected packages: typed-ast, mypy-extensions, mypy\n",
            "Successfully installed mypy-0.800 mypy-extensions-0.4.3 typed-ast-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WQsk_-pIw0p",
        "outputId": "5a7cfcb7-e285-4619-d357-3d61e463874f"
      },
      "source": [
        "pip install python-docx\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.6/dist-packages (0.8.10)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx) (4.2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZx6D4J_IzgA",
        "outputId": "7ede8b30-2f79-4b73-a7a1-7bf878b491fe"
      },
      "source": [
        "pip install nltk\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJw3JxjAIzrk",
        "outputId": "3d8e4401-4241-4104-ee59-71ee6a40dc26"
      },
      "source": [
        "pip install tika\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (51.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp36-none-any.whl size=32885 sha256=cd6847c3920e505c7915dd5dbd2c2ec7e6a54340ea041c90e1775565d301ae3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvcPyifuI5Yh",
        "outputId": "6d4020fb-81df-472d-8cb0-ebae21e360bf"
      },
      "source": [
        "pip install spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (51.3.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFEqFNCFI5zy"
      },
      "source": [
        "pip install numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV-Y8rV4I998"
      },
      "source": [
        "pip install pandas "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu36ENc0JAnA"
      },
      "source": [
        "pip install gensim "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kORWDHmJCYG"
      },
      "source": [
        "pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vNUJc-xrvwI"
      },
      "source": [
        "def customFilter(token):\n",
        "    customized_stop_words = [\n",
        "        \"work\",\n",
        "        \"experience\",\n",
        "        \"education\",\n",
        "        \"mobile\",\n",
        "        \"email\",\n",
        "        \"number\",\n",
        "        \"num\",\n",
        "        \"professional\",\n",
        "        \"career\",\n",
        "        \"history\",\n",
        "        \"histories\",\n",
        "        \"skill\",\n",
        "        \"skills\",\n",
        "        \"activity\",\n",
        "        \"activities\",\n",
        "        \"curriculum\",\n",
        "        \"tool\",\n",
        "        \"tools\",\n",
        "        \"language\",\n",
        "        \"languages\",\n",
        "        \"profile\",\n",
        "        \"qualification\",\n",
        "        \"qualifications\",\n",
        "        \"certificate\",\n",
        "        \"certificates\",\n",
        "        \"certifications\",\n",
        "        \"certification\",\n",
        "        \"information\",\n",
        "        \"intern\",\n",
        "        \"volunteer\",\n",
        "        \"award\",\n",
        "        \"awards\",\n",
        "        \"jan\",\n",
        "        \"feb\",\n",
        "        \"mar\",\n",
        "        \"apr\",\n",
        "        \"may\",\n",
        "        \"jun\",\n",
        "        \"jul\",\n",
        "        \"aug\",\n",
        "        \"sep\",\n",
        "        \"oct\",\n",
        "        \"nov\",\n",
        "        \"dec\",\n",
        "    ]\n",
        "    token_lower = token.lemma_.lower()\n",
        "    return all(\n",
        "        [\n",
        "            not token.is_space,  # not space\n",
        "            not token.is_punct,  # not punct\n",
        "            not token.is_bracket,  # not bracket\n",
        "            not token.is_quote,  # not quote\n",
        "            not token.is_currency,  # not currency\n",
        "            not token.like_num,  # not number\n",
        "            not token.like_url,  # not url\n",
        "            not token.like_email,  # not email\n",
        "            not token.is_oov,  # not out of vocab\n",
        "            not token.is_stop,  # not a stopword\n",
        "            token.is_alpha,  # is alphabetical\n",
        "            token.has_vector,\n",
        "            token_lower not in customized_stop_words,\n",
        "            token.pos_ not in [\"ADV\", \"ADJ\", \"INTJ\", \"PART\", \"PRON\", \"X\"],\n",
        "            token.ent_type_\n",
        "            not in [\n",
        "                \"PERSON\",\n",
        "                \"ORG\",\n",
        "                \"DATE\",\n",
        "                \"CARDINAL\",\n",
        "                \"TIME\",\n",
        "            ],  # not amongst these categories\n",
        "        ]\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjtQoY_zrcWO"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import docx\n",
        "\n",
        "from typing import *\n",
        "\n",
        "\n",
        "\n",
        "def loadDefaultNLP(is_big: bool = True) -> Any:\n",
        "    \"\"\"\n",
        "    Function to load the default SpaCy nlp model into self.nlp\n",
        "    :param is_big: if True, uses a large vocab set, else a small one\n",
        "    :returns: nlp: a SpaCy nlp model\n",
        "    \"\"\"\n",
        "\n",
        "    def segment_on_newline(doc):\n",
        "        for token in doc[:-1]:\n",
        "            if token.text.endswith(\"\\n\"):\n",
        "                doc[token.i + 1].is_sent_start = True\n",
        "        return doc\n",
        "\n",
        "    if is_big:\n",
        "        nlp = spacy.load(\"en_core_web_lg\")\n",
        "    else:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    nlp.add_pipe(segment_on_newline, before=\"parser\")\n",
        "    return nlp\n",
        "\n",
        "\n",
        "def countWords(line: str) -> int:\n",
        "    \"\"\"\n",
        "    Counts the numbers of words in a line\n",
        "    :param line: line to count\n",
        "    :return count: num of lines\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    is_space = False\n",
        "    for c in line:\n",
        "        is_not_char = not c.isspace()\n",
        "        if is_space and is_not_char:\n",
        "            count += 1\n",
        "        is_space = not is_not_char\n",
        "    return count\n",
        "\n",
        "\n",
        "def getAllTokensAndChunks(doc) -> Tuple[Set[Any], Set[Any]]:\n",
        "    \"\"\"\n",
        "    Converts a spacy doc into tokens and chunks. Tokens and chunks pass through a customFilter first\n",
        "    :param doc: a SpaCy doc\n",
        "    :returns: seen_chunks_words: set of strings seen\n",
        "    :returns: all_tokens_chunks: set of all tokens and chunks found\n",
        "    \"\"\"\n",
        "    # used to test duplicate words/chunks\n",
        "    seen_chunks_words = set()\n",
        "    # collate all words/chunks\n",
        "    all_tokens_chunks = set()\n",
        "    # generate all 1-gram tokens\n",
        "    for token in doc:\n",
        "        w = token.lemma_.lower()\n",
        "        if (w not in seen_chunks_words) and customFilter(token):\n",
        "            all_tokens_chunks.add(token)\n",
        "            seen_chunks_words.add(w)\n",
        "\n",
        "    # generate all n-gram tokens\n",
        "    for chunk in doc.noun_chunks:\n",
        "        c = chunk.lemma_.lower()\n",
        "        if (\n",
        "            len(chunk) > 1\n",
        "            and (c not in seen_chunks_words)\n",
        "            and all(customFilter(token) for token in chunk)\n",
        "        ):\n",
        "            all_tokens_chunks.add(chunk)\n",
        "            seen_chunks_words.add(c)\n",
        "\n",
        "    return seen_chunks_words, all_tokens_chunks\n",
        "\n",
        "\n",
        "def findDocumentsRecursive(base_dir: str) -> Optional[List[str]]:\n",
        "    \"\"\"\n",
        "    Recursively get all documents from `base_dir`\n",
        "    :param base_dir: base directory of documents\n",
        "    :returns out: a list of full file names of the documents\n",
        "    \"\"\"\n",
        "    out: List[str] = []\n",
        "\n",
        "    # check if base_dir is a proper dir\n",
        "    if not os.path.isdir(base_dir):\n",
        "        return None\n",
        "\n",
        "    for d in os.listdir(base_dir):\n",
        "        full_path = os.path.join(base_dir, d)\n",
        "        if os.path.isdir(full_path):\n",
        "            out.extend(findDocumentsRecursive(full_path))\n",
        "        else:\n",
        "            for end in (\".pdf\", \".docx\"):\n",
        "                if full_path.endswith(end):\n",
        "                    out.append(full_path)\n",
        "    return out\n",
        "\n",
        "\n",
        "def generateDFFromData(\n",
        "    data: Dict[Any, Any],\n",
        "    filename: str,\n",
        "    save_csv: bool = False\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates DF for model creation\n",
        "    :param data: dictionary of data\n",
        "    :param filename: what to save model as\n",
        "    :param save_csv: whether to save the model as csv\n",
        "    :returns data_df: the model df\n",
        "    \"\"\"\n",
        "    data_df = pd.DataFrame(data=data)\n",
        "    data_df.sort_values(by=[\"score\"], ascending=False, inplace=True)\n",
        "    data_df.reset_index(inplace=True)\n",
        "    if save_csv:\n",
        "        data_df.to_csv(filename)\n",
        "    return data_df\n",
        "\n",
        "\n",
        "def getDocxText(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the text from a docx file\n",
        "    :param filename: docx file\n",
        "    :returns fullText: text of file\n",
        "    \"\"\"\n",
        "    doc = docx.Document(filename)\n",
        "    fullText = []\n",
        "    for para in doc.paragraphs:\n",
        "        txt = para.text\n",
        "        fullText.append(txt)\n",
        "    return \"\\n\".join(fullText)\n",
        "\n",
        "\n",
        "def getPDFText(filename: str, parser) -> str:\n",
        "    \"\"\"\n",
        "    Get the text from a pdf file\n",
        "    :param filename: pdf file\n",
        "    :param parser: pdf parser\n",
        "    :returns fullText: text of file\n",
        "    \"\"\"\n",
        "    raw = parser.from_file(filename)\n",
        "    new_text = raw[\"content\"]\n",
        "    if \"title\" in raw[\"metadata\"]:\n",
        "        title = raw[\"metadata\"][\"title\"]\n",
        "        new_text = new_text.replace(title, \"\")\n",
        "    return new_text\n",
        "\n",
        "\n",
        "def loadDocumentIntoSpacy(f: str, parser, spacy_nlp) -> Optional[Tuple[Any, str]]:\n",
        "    \"\"\"\n",
        "    Convert file into spacy Document\n",
        "    :param f: filename\n",
        "    :param parser: pdf_parser\n",
        "    :param spacy_nlp: nlp model\n",
        "    :returns nlp_doc: nlp doc\n",
        "    :returns new_text: text of file\n",
        "    \"\"\"\n",
        "    if f.endswith(\".pdf\"):\n",
        "        new_text = getPDFText(f, parser)\n",
        "    elif f.endswith(\".docx\"):\n",
        "        new_text = getDocxText(f)\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "    # new_text = \"\\n\".join(\n",
        "    #     [line.strip() for line in new_text.split(\"\\n\") if len(line) > 1]\n",
        "    # )\n",
        "    new_text = re.sub(\"\\n{3,}\", \"\\n\", new_text)\n",
        "    new_text = str(bytes(new_text, \"utf-8\").replace(b\"\\xe2\\x80\\x93\", b\"\"), \"utf-8\")\n",
        "    # convert to spacy doc\n",
        "    return spacy_nlp(new_text), new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXOVoxP4sEJO"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from datetime import datetime\n",
        "from dateutil import relativedelta\n",
        "from typing import *\n",
        "\n",
        "WORDS_LIST = {\n",
        "    \"Work\": [\"(Work|WORK)\", \"(Experience(s?)|EXPERIENCE(S?))\", \"(History|HISTORY)\"],\n",
        "    \"Education\": [\"(Education|EDUCATION)\", \"(Qualifications|QUALIFICATIONS)\"],\n",
        "    \"Skills\": [\n",
        "        \"(Skills|SKILLS)\",\n",
        "        \"(Proficiency|PROFICIENCY)\",\n",
        "        \"LANGUAGE\",\n",
        "        \"CERTIFICATION\",\n",
        "    ],\n",
        "    \"Projects\": [\"(Projects|PROJECTS)\"],\n",
        "    \"Activities\": [\"(Leadership|LEADERSHIP)\", \"(Activities|ACTIVITIES)\"],\n",
        "}\n",
        "\n",
        "main_skills = \"\"\n",
        "\n",
        "class InfoExtractor:\n",
        "    \"\"\"\n",
        "    Extracts key information from resumes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spacy_nlp_model, parser):\n",
        "        self.nlp = spacy_nlp_model\n",
        "        self.parser = parser\n",
        "\n",
        "    def extractFromFile(self, filename):\n",
        "        doc, text = loadDocumentIntoSpacy(filename, self.parser, self.nlp)\n",
        "        skills = self.extractFromText(doc, text, filename)\n",
        "        return skills\n",
        "\n",
        "    def extractFromText(self, doc, text, filename):\n",
        "        name = InfoExtractor.findName(doc, filename)\n",
        "        if name is None:\n",
        "            name = \"\"\n",
        "        email = InfoExtractor.findEmail(doc)\n",
        "        if email is None:\n",
        "            email = \"\"\n",
        "        number = InfoExtractor.findNumber(doc)\n",
        "        if number is None:\n",
        "            number = \"\"\n",
        "        city = InfoExtractor.findCity(doc)\n",
        "        if city is None:\n",
        "            city = \"\"\n",
        "        categories = InfoExtractor.extractCategories(text)\n",
        "        workAndEducation = InfoExtractor.findWorkAndEducation(\n",
        "            categories, doc, text, name\n",
        "        )\n",
        "        totalWorkExperience = InfoExtractor.getTotalExperienceFormatted(\n",
        "            workAndEducation[\"Work\"]\n",
        "        )\n",
        "        totalEducationExperience = InfoExtractor.getTotalExperienceFormatted(\n",
        "            workAndEducation[\"Education\"]\n",
        "        )\n",
        "        allSkills = \", \".join(InfoExtractor.extractSkills(doc))\n",
        "        \n",
        "        #print(\"\\nWork Experience:\")\n",
        "        #print(totalWorkExperience)\n",
        "        #for w in workAndEducation[\"Work\"]:\n",
        "            #print(\" - \" + w)\n",
        "        #print(\"\\nEducation:\")\n",
        "        #print(totalEducationExperience)\n",
        "        #for e in workAndEducation[\"Education\"]:\n",
        "            #print(\" - \" + e)\n",
        "        #print(\"\\nSkills:\")\n",
        "        #print(allSkills)\n",
        "        \n",
        "        return [allSkills,totalWorkExperience]\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def extractSkills(doc) -> List[str]:\n",
        "        \"\"\"\n",
        "        Helper function to extract skills from spacy nlp text\n",
        "\n",
        "        :param doc: object of `spacy.tokens.doc.Doc`\n",
        "        :return: list of skills extracted\n",
        "        \"\"\"\n",
        "        #print(doc)\n",
        "        tokens = [token.text for token in doc if not token.is_stop]\n",
        "        data = pd.read_csv(\"./skills.csv\")\n",
        "    \n",
        "        skills = list(data.columns.values)\n",
        "        skillset = []\n",
        "        # check for one-grams\n",
        "        for token in tokens:\n",
        "            if token.lower() in skills:\n",
        "                skillset.append(token)\n",
        "\n",
        "        # check for bi-grams and tri-grams\n",
        "        for token in doc.noun_chunks:\n",
        "            token = token.text.lower().strip()\n",
        "            if token in skills:\n",
        "                skillset.append(token)\n",
        "        return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
        "\n",
        "    @staticmethod\n",
        "    def extractCategories(text) -> Dict[str, List[Tuple[int, int]]]:\n",
        "       \n",
        "        data = defaultdict(list)\n",
        "        page_count = 0\n",
        "        prev_count = 0\n",
        "        prev_line = None\n",
        "        prev_k = None\n",
        "        for line in text.split(\"\\n\"):\n",
        "            line = re.sub(r\"\\s+?\", \" \", line).strip()\n",
        "            for (k, wl) in WORDS_LIST.items():\n",
        "                # for each word in the list\n",
        "                for w in wl:\n",
        "                    # if category has not been found and not a very long line\n",
        "                    # - long line likely not a category\n",
        "                    if countWords(line) < 10:\n",
        "                        match = re.findall(w, line)\n",
        "                        if match:\n",
        "                            size = page_count - prev_count\n",
        "                            # append previous\n",
        "                            if prev_k is not None:\n",
        "                                data[prev_k].append((size, prev_count, prev_line))\n",
        "                            prev_count = page_count\n",
        "                            prev_k = k\n",
        "                            prev_line = line\n",
        "            page_count += 1\n",
        "\n",
        "        # last item\n",
        "        if prev_k is not None:\n",
        "            size = page_count - prev_count - 1 # -1 cuz page_count += 1 on prev line\n",
        "            data[prev_k].append((size, prev_count, prev_line))\n",
        "\n",
        "        # choose the biggest category (reduce false positives)\n",
        "        for k in data:\n",
        "            if len(data[k]) >= 2:\n",
        "                data[k] = [max(data[k], key=lambda x: x[0])]\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def findWorkAndEducation(categories, doc, text, name) -> Dict[str, List[str]]:\n",
        "        inv_data = {v[0][1]: (v[0][0], k) for k, v in categories.items()}\n",
        "        line_count = 0\n",
        "        exp_list = defaultdict(list)\n",
        "        name = name.lower()\n",
        "\n",
        "        current_line = None\n",
        "        is_dot = False\n",
        "        is_space = True\n",
        "        continuation_sent = []\n",
        "        first_line = None\n",
        "        unique_char_regex = \"[^\\sA-Za-z0-9\\.\\/\\(\\)\\,\\-\\|]+\"\n",
        "\n",
        "        for line in text.split(\"\\n\"):\n",
        "            line = re.sub(r\"\\s+\", \" \", line).strip()\n",
        "            match = re.search(r\"^.*:\", line)\n",
        "            if match:\n",
        "                line = line[match.end() :].strip()\n",
        "\n",
        "            # get first non-space line for filtering since\n",
        "            # sometimes it might be a page header\n",
        "            if line and first_line is None:\n",
        "                first_line = line\n",
        "\n",
        "            # update line_countfirst since there are `continue`s below\n",
        "            line_count += 1\n",
        "            if (line_count - 1) in inv_data:\n",
        "                current_line = inv_data[line_count - 1][1]\n",
        "            # contains a full-blown state-machine for filtering stuff\n",
        "            elif current_line == \"Work\":\n",
        "                if line:\n",
        "                    # if name is inside, skip\n",
        "                    if name == line:\n",
        "                        continue\n",
        "                    # if like first line of resume, skip\n",
        "                    if line == first_line:\n",
        "                        continue\n",
        "                    # check if it's not a list with some unique character as list bullet\n",
        "                    has_dot = re.findall(unique_char_regex, line[:5])\n",
        "                    # if last paragraph is a list item\n",
        "                    if is_dot:\n",
        "                        # if this paragraph is not a list item and the previous line is a space\n",
        "                        if not has_dot and is_space:\n",
        "                            if line[0].isupper() or re.findall(r\"^\\d+\\.\", line[:5]):\n",
        "                                exp_list[current_line].append(line)\n",
        "                                is_dot = False\n",
        "\n",
        "                    else:\n",
        "                        if not has_dot and (\n",
        "                            line[0].isupper() or re.findall(r\"^\\d+\\.\", line[:5])\n",
        "                        ):\n",
        "                            exp_list[current_line].append(line)\n",
        "                            is_dot = False\n",
        "                    if has_dot:\n",
        "                        is_dot = True\n",
        "                    is_space = False\n",
        "                else:\n",
        "                    is_space = True\n",
        "            elif current_line == \"Education\":\n",
        "                if line:\n",
        "                    # if not like first line\n",
        "                    if line == first_line:\n",
        "                        continue\n",
        "                    line = re.sub(unique_char_regex, '', line[:5]) + line[5:]\n",
        "                    if len(line) < 12:\n",
        "                        continuation_sent.append(line)\n",
        "                    else:\n",
        "                        if continuation_sent:\n",
        "                            continuation_sent.append(line)\n",
        "                            line = \" \".join(continuation_sent)\n",
        "                            continuation_sent = []\n",
        "                        exp_list[current_line].append(line)\n",
        "\n",
        "        return exp_list\n",
        "\n",
        "    @staticmethod\n",
        "    def findNumber(doc) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Helper function to extract number from nlp doc\n",
        "        :param doc: SpaCy Doc of text\n",
        "        :return: int:number if found, else None\n",
        "        \"\"\"\n",
        "        for sent in doc.sents:\n",
        "            num = re.findall(r\"\\(?\\+?\\d+\\)?\\d+(?:[- \\)]+\\d+)*\", sent.text)\n",
        "            if num:\n",
        "                for n in num:\n",
        "                    if len(n) >= 8 and (\n",
        "                        not re.findall(r\"^[0-9]{2,4} *-+ *[0-9]{2,4}$\", n)\n",
        "                    ):\n",
        "                        return n\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def findEmail(doc) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Helper function to extract email from nlp doc\n",
        "        :param doc: SpaCy Doc of text\n",
        "        :return: str:email if found, else None\n",
        "        \"\"\"\n",
        "        for token in doc:\n",
        "            if token.like_email:\n",
        "                return token.text\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def findCity(doc) -> Optional[str]:\n",
        "        counter = Counter()\n",
        "        \"\"\"\n",
        "        Helper function to extract most likely City/Country from nlp doc\n",
        "        :param doc: SpaCy Doc of text\n",
        "        :return: str:city/country if found, else None\n",
        "        \"\"\"\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"GPE\":\n",
        "                counter[ent.text] += 1\n",
        "\n",
        "        if len(counter) >= 1:\n",
        "            return counter.most_common(1)[0][0]\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def findName(doc, filename) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Helper function to extract name from nlp doc\n",
        "        :param doc: SpaCy Doc of text\n",
        "        :param filename: used as backup if NE cannot be found\n",
        "        :return: str:NAME_PATTERN if found, else None\n",
        "        \"\"\"\n",
        "        to_chain = False\n",
        "        all_names = []\n",
        "        person_name = None\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"PERSON\":\n",
        "                if not to_chain:\n",
        "                    person_name = ent.text.strip()\n",
        "                    to_chain = True\n",
        "                else:\n",
        "                    person_name = person_name + \" \" + ent.text.strip()\n",
        "            elif ent.label_ != \"PERSON\":\n",
        "                if to_chain:\n",
        "                    all_names.append(person_name)\n",
        "                    person_name = None\n",
        "                    to_chain = False\n",
        "        if all_names:\n",
        "            return all_names[0]\n",
        "        else:\n",
        "            try:\n",
        "                base_name_wo_ex = os.path.splitext(os.path.basename(filename))[0]\n",
        "                return base_name_wo_ex + \" (from filename)\"\n",
        "            except:\n",
        "                return None\n",
        "\n",
        "    @staticmethod\n",
        "    def getNumberOfMonths(datepair) -> int:\n",
        "        \"\"\"\n",
        "        Helper function to extract total months of experience from a resume\n",
        "\n",
        "        :param date1: Starting date\n",
        "        :param date2: Ending date\n",
        "        :return: months of experience from date1 to date2\n",
        "        \"\"\"\n",
        "        # if years\n",
        "        # if years\n",
        "        date2_parsed = False\n",
        "        if datepair.get(\"fh\", None) is not None:\n",
        "            gap = datepair[\"fh\"]\n",
        "        else:\n",
        "            gap = \"\"\n",
        "        try:\n",
        "            present_vocab = (\"present\", \"date\", \"now\")\n",
        "            if \"syear\" in datepair:\n",
        "                date1 = datepair[\"fyear\"]\n",
        "                date2 = datepair[\"syear\"]\n",
        "\n",
        "                if date2.lower() in present_vocab:\n",
        "                    date2 = datetime.now()\n",
        "                    date2_parsed = True\n",
        "\n",
        "                try:\n",
        "                    if not date2_parsed:\n",
        "                        date2 = datetime.strptime(str(date2), \"%Y\")\n",
        "                    date1 = datetime.strptime(str(date1), \"%Y\")\n",
        "                except:\n",
        "                    pass\n",
        "            elif \"smonth_num\" in datepair:\n",
        "                date1 = datepair[\"fmonth_num\"]\n",
        "                date2 = datepair[\"smonth_num\"]\n",
        "\n",
        "                if date2.lower() in present_vocab:\n",
        "                    date2 = datetime.now()\n",
        "                    date2_parsed = True\n",
        "\n",
        "                for stype in (\"%m\" + gap + \"%Y\", \"%m\" + gap + \"%y\"):\n",
        "                    try:\n",
        "                        if not date2_parsed:\n",
        "                            date2 = datetime.strptime(str(date2), stype)\n",
        "                        date1 = datetime.strptime(str(date1), stype)\n",
        "                        break\n",
        "                    except:\n",
        "                        pass\n",
        "            else:\n",
        "                date1 = datepair[\"fmonth\"]\n",
        "                date2 = datepair[\"smonth\"]\n",
        "\n",
        "                if date2.lower() in present_vocab:\n",
        "                    date2 = datetime.now()\n",
        "                    date2_parsed = True\n",
        "\n",
        "                for stype in (\n",
        "                    \"%b\" + gap + \"%Y\",\n",
        "                    \"%b\" + gap + \"%y\",\n",
        "                    \"%B\" + gap + \"%Y\",\n",
        "                    \"%B\" + gap + \"%y\",\n",
        "                ):\n",
        "                    try:\n",
        "                        if not date2_parsed:\n",
        "                            date2 = datetime.strptime(str(date2), stype)\n",
        "                        date1 = datetime.strptime(str(date1), stype)\n",
        "                        break\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            months_of_experience = relativedelta.relativedelta(date2, date1)\n",
        "            months_of_experience = (\n",
        "                months_of_experience.years * 12 + months_of_experience.months\n",
        "            )\n",
        "            return months_of_experience\n",
        "        except Exception as e:\n",
        "            return 0\n",
        "\n",
        "    @staticmethod\n",
        "    def getTotalExperience(experience_list) -> int:\n",
        "        \"\"\"\n",
        "        Wrapper function to extract total months of experience from a resume\n",
        "\n",
        "        :param experience_list: list of experience text extracted\n",
        "        :return: total months of experience\n",
        "        \"\"\"\n",
        "        exp_ = []\n",
        "        for line in experience_list:\n",
        "            line = line.lower().strip()\n",
        "            # have to split search since regex OR does not capture on a first-come-first-serve basis\n",
        "            experience = re.search(\n",
        "                r\"(?P<fyear>\\d{4})\\s*(\\s|-|to)\\s*(?P<syear>\\d{4}|present|date|now)\",\n",
        "                line,\n",
        "                re.I,\n",
        "            )\n",
        "            if experience:\n",
        "                d = experience.groupdict()\n",
        "                exp_.append(d)\n",
        "                continue\n",
        "\n",
        "            experience = re.search(\n",
        "                r\"(?P<fmonth>\\w+(?P<fh>.)\\d+)\\s*(\\s|-|to)\\s*(?P<smonth>\\w+(?P<sh>.)\\d+|present|date|now)\",\n",
        "                line,\n",
        "                re.I,\n",
        "            )\n",
        "            if experience:\n",
        "                d = experience.groupdict()\n",
        "                exp_.append(d)\n",
        "                continue\n",
        "\n",
        "            experience = re.search(\n",
        "                r\"(?P<fmonth_num>\\d+(?P<fh>.)\\d+)\\s*(\\s|-|to)\\s*(?P<smonth_num>\\d+(?P<sh>.)\\d+|present|date|now)\",\n",
        "                line,\n",
        "                re.I,\n",
        "            )\n",
        "            if experience:\n",
        "                d = experience.groupdict()\n",
        "                exp_.append(d)\n",
        "                continue\n",
        "        experience_num_list = [InfoExtractor.getNumberOfMonths(i) for i in exp_]\n",
        "        total_experience_in_months = sum(experience_num_list)\n",
        "        return total_experience_in_months\n",
        "\n",
        "    @staticmethod\n",
        "    def getTotalExperienceFormatted(exp_list) -> str:\n",
        "        months = InfoExtractor.getTotalExperience(exp_list)\n",
        "        if months < 12:\n",
        "            return str(months) + \" months\"\n",
        "        years = months // 12\n",
        "        months = months % 12\n",
        "        return str(years) + \" years \" + str(months) + \" months\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHgYbLNVtgqI"
      },
      "source": [
        "pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euYgxc9ctfSP"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "resumeList = ['./kush.pdf','./marco.pdf','./nava.pdf','./parth.pdf','./pulkit.pdf', './ayesha.pdf','./rakshith.pdf']\n",
        "main_list = []\n",
        "\n",
        "def listToString(s):  \n",
        "    \n",
        "    # initialize an empty string \n",
        "    str1 = \"\"  \n",
        "    \n",
        "    # traverse in the string   \n",
        "    for ele in s:  \n",
        "        str1 += ele\n",
        "        str1 += \" \"   \n",
        "    \n",
        "    # return string   \n",
        "    return str1\n",
        "\n",
        "for i in resumeList:\n",
        " \n",
        "  filename = i\n",
        "  from tika import parser\n",
        "\n",
        "  nlp = loadDefaultNLP(is_big=False)\n",
        "  infoExtractor = InfoExtractor(nlp, parser)\n",
        "  skills = infoExtractor.extractFromFile(filename)\n",
        "\n",
        "  job_desc = \"Google HTML CSS React.js Hard-Working Anguar.js JavaScript Software Engineer API\"\n",
        "\n",
        "  #ALL CATEGORIES\n",
        "\n",
        "  text = [skills[0],job_desc]\n",
        "\n",
        "  cv = CountVectorizer()\n",
        "\n",
        "  count_matrix = cv.fit_transform(text)\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "  match_percent = cosine_similarity(count_matrix)[0][1]*100\n",
        "  match_percent = round(match_percent,2)\n",
        "\n",
        "  if('years' in skills[1]):\n",
        "    main_list.append([filename,match_percent,int(skills[1][0])*12 + int(skills[1][8])])\n",
        "\n",
        "\n",
        "  elif('months' in skills[1]):\n",
        "    main_list.append([filename,match_percent,int(skills[1][0])])\n",
        "\n",
        "  #print( filename + \" This resume matches about \" + str(match_percent)  + \" percent to the job profile. \\n\" ) \n",
        "\n",
        "#print(main_list)\n",
        "main_list.sort(key = lambda x: x[1])\n",
        "main_list.reverse()\n",
        "for i in main_list:\n",
        "  print(i)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}